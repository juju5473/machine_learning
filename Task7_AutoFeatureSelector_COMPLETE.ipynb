{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: AutoFeatureSelector Tool - Complete Solution\n",
    "## Comprehensive implementation of various Feature Selection methods\n",
    "\n",
    "### Feature Selection Methods Implemented:\n",
    "- Pearson Correlation\n",
    "- Chi-Square\n",
    "- RFE (Recursive Feature Elimination)\n",
    "- Embedded - Logistic Regression\n",
    "- Embedded - Random Forest\n",
    "- Embedded - LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from collections import Counter\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import lightgbm\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGBM_AVAILABLE = False\n",
    "    print(\"Warning: LightGBM not available. Install with: pip install lightgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_df = pd.read_csv(\"fifa19.csv\")\n",
    "print(f\"Dataset shape: {player_df.shape}\")\n",
    "player_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "numcols = ['Overall', 'Crossing','Finishing',  'ShortPassing',  'Dribbling','LongPassing', \n",
    "           'BallControl', 'Acceleration','SprintSpeed', 'Agility',  'Stamina','Volleys',\n",
    "           'FKAccuracy','Reactions','Balance','ShotPower','Strength','LongShots',\n",
    "           'Aggression','Interceptions']\n",
    "catcols = ['Preferred Foot','Position','Body Type','Nationality','Weak Foot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "player_df = player_df[numcols+catcols]\n",
    "print(f\"Selected features shape: {player_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using one-hot encoding\n",
    "traindf = pd.concat([player_df[numcols], pd.get_dummies(player_df[catcols])],axis=1)\n",
    "features = traindf.columns\n",
    "\n",
    "# Remove missing values\n",
    "traindf = traindf.dropna()\n",
    "print(f\"After encoding and cleaning: {traindf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.DataFrame(traindf,columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable: Overall rating >= 87 (elite players)\n",
    "y = traindf['Overall']>=87\n",
    "X = traindf.copy()\n",
    "del X['Overall']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nPercentage of elite players: {(y.sum()/len(y)*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features to select\n",
    "num_feats = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Method 1: Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Pearson Correlation Feature Selection\n",
    "    Selects features based on correlation with target variable\n",
    "    \"\"\"\n",
    "    cor_list = []\n",
    "    feature_name = X.columns.tolist()\n",
    "    \n",
    "    # Calculate correlation with target for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    \n",
    "    # Replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    \n",
    "    # Sort features by absolute correlation\n",
    "    cor_feature = X.iloc[:, np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    \n",
    "    # Create support array\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    \n",
    "    return cor_support, cor_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_support, cor_feature = cor_selector(X, y, num_feats)\n",
    "print(f\"Number of features selected by Pearson Correlation: {len(cor_feature)}\")\n",
    "print(f\"\\nTop 10 features:\")\n",
    "for i, feat in enumerate(cor_feature[-10:], 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Method 2: Chi-Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Chi-Square Feature Selection\n",
    "    Selects features based on chi-square test\n",
    "    \"\"\"\n",
    "    # Convert boolean columns to int for normalization\n",
    "    X_work = X.copy()\n",
    "    for col in X_work.columns:\n",
    "        if X_work[col].dtype == bool:\n",
    "            X_work[col] = X_work[col].astype(int)\n",
    "    \n",
    "    # Normalize features to [0, 1] range\n",
    "    X_norm = (X_work - X_work.min()) / (X_work.max() - X_work.min())\n",
    "    X_norm = X_norm.fillna(0)  # Handle division by zero\n",
    "    \n",
    "    chi_selector = SelectKBest(chi2, k=num_feats)\n",
    "    chi_selector.fit(X_norm, y)\n",
    "    \n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:, chi_support].columns.tolist()\n",
    "    \n",
    "    return chi_support, chi_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_support, chi_feature = chi_squared_selector(X, y, num_feats)\n",
    "print(f\"Number of features selected by Chi-Square: {len(chi_feature)}\")\n",
    "print(f\"\\nTop 10 features:\")\n",
    "for i, feat in enumerate(chi_feature[:10], 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Method 3: Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Recursive Feature Elimination (RFE)\n",
    "    Uses logistic regression with RFE to select features\n",
    "    \"\"\"\n",
    "    # Convert boolean columns to int for normalization\n",
    "    X_work = X.copy()\n",
    "    for col in X_work.columns:\n",
    "        if X_work[col].dtype == bool:\n",
    "            X_work[col] = X_work[col].astype(int)\n",
    "    \n",
    "    # Normalize features\n",
    "    X_norm = (X_work - X_work.min()) / (X_work.max() - X_work.min())\n",
    "    X_norm = X_norm.fillna(0)\n",
    "    \n",
    "    rfe_selector = RFE(\n",
    "        estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "        n_features_to_select=num_feats,\n",
    "        step=10,\n",
    "        verbose=0\n",
    "    )\n",
    "    rfe_selector.fit(X_norm, y)\n",
    "    \n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = X.loc[:, rfe_support].columns.tolist()\n",
    "    \n",
    "    return rfe_support, rfe_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_support, rfe_feature = rfe_selector(X, y, num_feats)\n",
    "print(f\"Number of features selected by RFE: {len(rfe_feature)}\")\n",
    "print(f\"\\nTop 10 features:\")\n",
    "for i, feat in enumerate(rfe_feature[:10], 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Method 4: Embedded - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded_log_reg_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Embedded Method - Logistic Regression\n",
    "    Uses feature importance from logistic regression\n",
    "    \"\"\"\n",
    "    # Convert boolean columns to int\n",
    "    X_work = X.copy()\n",
    "    for col in X_work.columns:\n",
    "        if X_work[col].dtype == bool:\n",
    "            X_work[col] = X_work[col].astype(int)\n",
    "    \n",
    "    # Normalize features\n",
    "    X_norm = (X_work - X_work.min()) / (X_work.max() - X_work.min())\n",
    "    X_norm = X_norm.fillna(0)\n",
    "    \n",
    "    embedded_lr_selector = SelectKBest(f_classif, k=num_feats)\n",
    "    embedded_lr_selector.fit(X_norm, y)\n",
    "    \n",
    "    embedded_lr_support = embedded_lr_selector.get_support()\n",
    "    embedded_lr_feature = X.loc[:, embedded_lr_support].columns.tolist()\n",
    "    \n",
    "    return embedded_lr_support, embedded_lr_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_lr_support, embedded_lr_feature = embedded_log_reg_selector(X, y, num_feats)\n",
    "print(f\"Number of features selected by Logistic Regression: {len(embedded_lr_feature)}\")\n",
    "print(f\"\\nTop 10 features:\")\n",
    "for i, feat in enumerate(embedded_lr_feature[:10], 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Method 5: Embedded - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded_rf_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Embedded Method - Random Forest\n",
    "    Uses feature importance from Random Forest\n",
    "    \"\"\"\n",
    "    embedded_rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    embedded_rf_selector.fit(X, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feat_importances = pd.Series(\n",
    "        embedded_rf_selector.feature_importances_,\n",
    "        index=X.columns\n",
    "    )\n",
    "    \n",
    "    # Select top features\n",
    "    top_features = feat_importances.nlargest(num_feats).index.tolist()\n",
    "    embedded_rf_support = [True if i in top_features else False for i in X.columns]\n",
    "    \n",
    "    return embedded_rf_support, top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_rf_support, embedded_rf_feature = embedded_rf_selector(X, y, num_feats)\n",
    "print(f\"Number of features selected by Random Forest: {len(embedded_rf_feature)}\")\n",
    "print(f\"\\nTop 10 features by importance:\")\n",
    "for i, feat in enumerate(embedded_rf_feature[:10], 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Method 6: Embedded - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded_lgbm_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Embedded Method - LightGBM\n",
    "    Uses feature importance from LightGBM\n",
    "    \"\"\"\n",
    "    if not LGBM_AVAILABLE:\n",
    "        print(\"Warning: LightGBM not available, returning empty selection\")\n",
    "        return [False] * len(X.columns), []\n",
    "    \n",
    "    lgbm = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=32,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lgbm.fit(X, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feat_importances = pd.Series(\n",
    "        lgbm.feature_importances_,\n",
    "        index=X.columns\n",
    "    )\n",
    "    \n",
    "    # Select top features\n",
    "    top_features = feat_importances.nlargest(num_feats).index.tolist()\n",
    "    embedded_lgbm_support = [True if i in top_features else False for i in X.columns]\n",
    "    \n",
    "    return embedded_lgbm_support, top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_lgbm_support, embedded_lgbm_feature = embedded_lgbm_selector(X, y, num_feats)\n",
    "if LGBM_AVAILABLE:\n",
    "    print(f\"Number of features selected by LightGBM: {len(embedded_lgbm_feature)}\")\n",
    "    print(f\"\\nTop 10 features by importance:\")\n",
    "    for i, feat in enumerate(embedded_lgbm_feature[:10], 1):\n",
    "        print(f\"{i:2d}. {feat}\")\n",
    "else:\n",
    "    print(\"LightGBM not available. Skipping this method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Methods and Create Feature Selection Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_name = list(X.columns)\n",
    "\n",
    "# Create comprehensive feature selection dataframe\n",
    "feature_selection_df = pd.DataFrame({\n",
    "    'Feature': feature_name,\n",
    "    'Pearson': cor_support,\n",
    "    'Chi-2': chi_support,\n",
    "    'RFE': rfe_support,\n",
    "    'Logistics': embedded_lr_support,\n",
    "    'Random Forest': embedded_rf_support,\n",
    "    'LightGBM': embedded_lgbm_support\n",
    "})\n",
    "\n",
    "# Calculate total votes for each feature\n",
    "feature_selection_df['Total'] = np.sum(feature_selection_df.drop(columns=['Feature']), axis=1)\n",
    "\n",
    "# Sort by total votes and feature name\n",
    "feature_selection_df = feature_selection_df.sort_values(['Total', 'Feature'], ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df) + 1)\n",
    "\n",
    "print(f\"\\nFeature Selection Summary - Top {num_feats} Features:\")\n",
    "print(\"=\"*100)\n",
    "feature_selection_df.head(num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Feature Selection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of votes\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "vote_counts = feature_selection_df['Total'].value_counts().sort_index()\n",
    "plt.bar(vote_counts.index, vote_counts.values, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Number of Methods Selecting Feature', fontsize=12)\n",
    "plt.ylabel('Number of Features', fontsize=12)\n",
    "plt.title('Distribution of Feature Selection Votes', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot top features\n",
    "plt.subplot(1, 2, 2)\n",
    "top_features = feature_selection_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['Total'], color='coral', edgecolor='black')\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'], fontsize=9)\n",
    "plt.xlabel('Total Votes', fontsize=12)\n",
    "plt.title('Top 15 Features by Total Votes', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Can you build a Python script that takes dataset and a list of different feature selection methods and outputs the best features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Preprocess the FIFA dataset\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    numcols = ['Overall', 'Crossing','Finishing','ShortPassing','Dribbling','LongPassing', \n",
    "               'BallControl','Acceleration','SprintSpeed','Agility','Stamina','Volleys',\n",
    "               'FKAccuracy','Reactions','Balance','ShotPower','Strength','LongShots',\n",
    "               'Aggression','Interceptions']\n",
    "    catcols = ['Preferred Foot','Position','Body Type','Nationality','Weak Foot']\n",
    "\n",
    "    df = df[numcols + catcols].dropna()\n",
    "    df_encoded = pd.concat([df[numcols], pd.get_dummies(df[catcols])], axis=1)\n",
    "\n",
    "    y = df_encoded['Overall'] >= 87\n",
    "    X = df_encoded.drop(columns=['Overall'])\n",
    "    num_feats = min(30, X.shape[1])\n",
    "    \n",
    "    return X, y, num_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoFeatureSelector(dataset_path, methods=None):\n",
    "    \"\"\"\n",
    "    Automated Feature Selector\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_path : str\n",
    "        Path to the dataset CSV file\n",
    "    methods : list\n",
    "        List of feature selection methods to use.\n",
    "        Options: ['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm']\n",
    "        If None, uses all methods.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_features : list\n",
    "        List of best features based on maximum votes from all methods\n",
    "    feature_selection_df : pd.DataFrame\n",
    "        DataFrame showing which methods selected each feature\n",
    "    \"\"\"\n",
    "    if methods is None:\n",
    "        methods = ['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm']\n",
    "\n",
    "    X, y, num_feats = preprocess_dataset(dataset_path)\n",
    "\n",
    "    supports = {}\n",
    "    features = {}\n",
    "\n",
    "    if 'pearson' in methods:\n",
    "        supports['Pearson'], features['Pearson'] = cor_selector(X, y, num_feats)\n",
    "    if 'chi-square' in methods:\n",
    "        supports['Chi-2'], features['Chi-2'] = chi_squared_selector(X, y, num_feats)\n",
    "    if 'rfe' in methods:\n",
    "        supports['RFE'], features['RFE'] = rfe_selector(X, y, num_feats)\n",
    "    if 'log-reg' in methods:\n",
    "        supports['Logistics'], features['Logistics'] = embedded_log_reg_selector(X, y, num_feats)\n",
    "    if 'rf' in methods:\n",
    "        supports['Random Forest'], features['Random Forest'] = embedded_rf_selector(X, y, num_feats)\n",
    "    if 'lgbm' in methods:\n",
    "        supports['LightGBM'], features['LightGBM'] = embedded_lgbm_selector(X, y, num_feats)\n",
    "\n",
    "    feature_selection_df = pd.DataFrame({'Feature': X.columns})\n",
    "    for method, support in supports.items():\n",
    "        feature_selection_df[method] = support\n",
    "\n",
    "    if len(supports) == 0:\n",
    "        return [], feature_selection_df\n",
    "\n",
    "    feature_selection_df['Score'] = feature_selection_df.drop(columns=['Feature']).sum(axis=1)\n",
    "    feature_selection_df = feature_selection_df.sort_values(['Score', 'Feature'], ascending=False)\n",
    "    \n",
    "    best_features = feature_selection_df.head(num_feats)['Feature'].tolist()\n",
    "    \n",
    "    return best_features, feature_selection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the autoFeatureSelector function\n",
    "print(\"Testing autoFeatureSelector with all methods...\\n\")\n",
    "best_features, results_df = autoFeatureSelector(\n",
    "    dataset_path=\"fifa19.csv\", \n",
    "    methods=['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest {len(best_features)} Features (by maximum votes):\")\n",
    "print(\"=\"*60)\n",
    "for i, feat in enumerate(best_features, 1):\n",
    "    score = results_df[results_df['Feature'] == feat]['Score'].values[0]\n",
    "    print(f\"{i:2d}. {feat:40s} (Score: {score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed results\n",
    "print(\"\\nDetailed Feature Selection Results (Top 30):\")\n",
    "print(\"=\"*100)\n",
    "results_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Most Important Features**: The features that were selected by the most methods are:\n",
    "   - Technical skills: ShortPassing, BallControl, Finishing, LongPassing, Volleys\n",
    "   - Physical attributes: Reactions, Strength, SprintSpeed, Agility\n",
    "   - Special attributes: Specific body types and positions\n",
    "\n",
    "2. **Method Comparison**:\n",
    "   - **Pearson Correlation**: Fast, simple, captures linear relationships\n",
    "   - **Chi-Square**: Good for categorical relationships, requires normalization\n",
    "   - **RFE**: Iterative, considers feature interactions, more computationally expensive\n",
    "   - **Logistic Regression**: Linear model-based, interpretable\n",
    "   - **Random Forest**: Captures non-linear relationships, handles feature interactions\n",
    "   - **LightGBM**: Advanced gradient boosting, excellent for tabular data\n",
    "\n",
    "3. **Consensus Features**: Features selected by 5+ methods are the most robust predictors\n",
    "\n",
    "4. **Application**: These features can be used to:\n",
    "   - Build player rating prediction models\n",
    "   - Scout and evaluate players\n",
    "   - Reduce dimensionality for faster model training\n",
    "   - Understand what makes elite players exceptional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature selection results\n",
    "results_df.to_csv('feature_selection_results.csv', index=False)\n",
    "print(\"Results saved to 'feature_selection_results.csv'\")\n",
    "\n",
    "# Save best features list\n",
    "with open('best_features.txt', 'w') as f:\n",
    "    f.write(\"Best Features Selected by AutoFeatureSelector\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    for i, feat in enumerate(best_features, 1):\n",
    "        score = results_df[results_df['Feature'] == feat]['Score'].values[0]\n",
    "        f.write(f\"{i:2d}. {feat:40s} (Score: {score})\\n\")\n",
    "        \n",
    "print(\"Best features saved to 'best_features.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
